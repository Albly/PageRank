import numpy as np
from numpy.linalg.linalg import norm
from numba import njit



@njit(fastmath=True)
def power_iteration(A, b_k):
    b_k = np.dot(A, b_k)
    b_k_norm = np.linalg.norm(b_k)
    b_k = b_k / b_k_norm
        
    return b_k


def power_method(A, num_simulations: int, create_history = False, autostop_threshold = 1e7):
    # Ideally choose a random vector
    # To decrease the chance that our vector
    # Is orthogonal to the eigenvector
    if create_history: 
        history_power = []
    
    b_k = np.random.rand(A.shape[1])

    for i in range(num_simulations):
        
        if create_history:
            history_power.append(b_k)

        # calculate the matrix-by-vector product Ab
        b_k1 = np.dot(A, b_k)

        # calculate the norm
        b_k1_norm = np.linalg.norm(b_k1)

        # re normalize the vector
        new_bk = b_k1 / b_k1_norm 
        
        if autostop_threshold != 0 :
            if np.linalg.norm(b_k - new_bk) < autostop_threshold:
                return new_bk

        b_k = new_bk 
    
    return b_k
